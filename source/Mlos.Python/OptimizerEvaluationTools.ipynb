{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "warnings.simplefilter(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this notebook is to demonstrate the use of the Optimizer Evaluation Tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer evaluation purpose\n",
    "\n",
    "The goal of optimizer evaluation is to learn how to best match an optimizer configuration to an optimization problem.\n",
    "\n",
    "## Optimizer evaluation strategy\n",
    "\n",
    "Broadly, the optimizer evaluation strategy hinges on characterizing the optimizers' performance on a variety of optimization problems, under a variety of configurations. This should allow us to:\n",
    "* find the strengths and limitations of the various optimizer configurations,\n",
    "* discard the dominated configurations,\n",
    "* ultimately match optimizer configurations with the problem.\n",
    "\n",
    "Note: No absolute scale of optimzier performance seems to have garnered consensus in the community. Thus, in this exercise we will compare the optimizers with each other. We can also compare them to some published results, though it might be good to replicate them first.\n",
    "\n",
    "A common way to compare multiple optimizers is to rank them for each problem given a particular budget (usually in terms of number of evaluations, though if we assume that the function is cheap to compute, optimizer resource consumption might be more relevant). Then we can report average rank across problems to compare the optimizers. Or we can do a matrix for each benchmark problem which optimizer dominates which.\n",
    "\n",
    "\n",
    "### Optimizer performance characteristics\n",
    "\n",
    "We aim to evaluate the following aspects of the optimizers' performance:\n",
    "* convergence - is the optimizer finding the optima, More formally, can the optimizer get within epsilon of the known optimum at all?\n",
    "* rate of convergence - how quickly is the optimizer finding the optima?\n",
    "* trajectory of convergence - is the optimizer improving rapidly at first, and then reaching a plateau, is it climbing steadily, or does it stagnate for a long time, before finally shooting up?\n",
    "* surrogate model goodness of fit - how well do the models fit the training data, validation data (out of bag samples), test data (observations gathered after the model was fit), random test data (random observations gathered after the model was fit). \n",
    "* computational cost - for now we can use the Tracer to capture runtime information, down the road we should monitor CPU, and memory utilization too.\n",
    "\n",
    "### What to measure\n",
    "\n",
    "ML metrics:\n",
    "* Convergence of the entire optimizer on synthetic and real data.\n",
    "* Models' goodness of fit on synthetic and real data, in both on both guided and unguided observations.\n",
    "* Utility function optimizers' performance on synthetic data.\n",
    "We shall measure and plot all of the above metrics as a function of the number of observations that the optimzier has been fit on.\n",
    "\n",
    "\n",
    "Resource consumption metrics:\n",
    "* Optimization Trace\n",
    "\n",
    "Additionally, we can serialize the models over time to later inspect how they evolve. And in case something goes wrong, we have snapshots leading up to the failure.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### Definition of optimum\n",
    "Various definitions of optimum are possible, and we have built consensus that the user should choose a definition suitable for their purpose. Here are some options:\n",
    "1. Best observation - simplest, but can be deceptive in presence of noise. \n",
    "2. Observations with the highest:\n",
    "    1. predicted mean performance\n",
    "    2. upper confidence bound on performance\n",
    "    3. lower confidence bound on performance\n",
    "3. Speculative optima - return configurations predicted by the surrogate model but not necessarily tested:\n",
    "    1. maximum predicted mean\n",
    "    2. maximum predicted upper confidence bound\n",
    "    3. maximum predicted lower confidence bound\n",
    "        \n",
    "Once we graduate to multi-objective optimization, we will need to build pareto frontiers from the above.\n",
    "<hr>\n",
    "\n",
    "\n",
    "### Selecting the optimizer configuration\n",
    "\n",
    "The ability of an optimizer to converge on an optimum is our fundamental requirement. From between the optimizers that can converge we can break ties using secondary criteria: rate of convergence, goodness of fit, computational complexity of the optimizer.\n",
    "\n",
    "### Troubleshooting the optimizers\n",
    "\n",
    "For the optimizers that do not converge, this framework should illuminate their modes of failure:\n",
    "* Is the model not fitting the data well?\n",
    "* Is the model fitting the data, but the exposed parameters don't affect performance, or is there too much noise?\n",
    "* If the model has strong goodness of fit, and performance is sensitive to the parameters, is the utility function optimizer up to the job? \n",
    "* Are we using the right utility function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeWarning",
     "evalue": "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0773b460043c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmlos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_values\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mglobal_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmlos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTracer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTracer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mglobal_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclare_singletons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\code\\github_mlos\\source\\mlos.python\\mlos\\Tracer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmlos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglobal_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m__init__.pxd\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeWarning\u001b[0m: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import mlos.global_values as global_values\n",
    "from mlos.Tracer import traced, Tracer\n",
    "\n",
    "global_values.declare_singletons()\n",
    "global_values.tracer = Tracer(actor_id=\"OptimizerEvaluation\", thread_id=0)\n",
    "\n",
    "# Let's prepare a directory for evaluation reports.\n",
    "#\n",
    "optimizer_results_root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\",\"..\", \"temp\"))\n",
    "if not os.path.exists(optimizer_results_root_dir):\n",
    "    os.mkdir(optimizer_results_root_dir)\n",
    "optimizer_results_folder_name = f\"optimization_started_on_{datetime.datetime.now().strftime('%Y_%m_%d_at_%H_%M_%S')}\"\n",
    "optimizer_results_dir = os.path.abspath(os.path.join(optimizer_results_root_dir, optimizer_results_folder_name))\n",
    "\n",
    "if not os.path.exists(optimizer_results_dir):\n",
    "    os.mkdir(optimizer_results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "Let's create an objective function taht we will be configuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.OptimizerEvaluationTools.ObjectiveFunctionConfigStore import objective_function_config_store\n",
    "from mlos.OptimizerEvaluationTools.ObjectiveFunctionFactory import ObjectiveFunctionFactory\n",
    "\n",
    "objective_function_config = objective_function_config_store.get_config_by_name('5_mutually_exclusive_polynomials')\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.input_domain_min = -20\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.input_domain_width = 40\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.coefficient_domain_min = 100\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.coefficient_domain_width = 10\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.include_noise = False\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.noice_coefficient_of_variation = 0.0\n",
    "\n",
    "print(\"Objective function config:\")\n",
    "print(objective_function_config.to_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function = ObjectiveFunctionFactory.create_objective_function(objective_function_config)\n",
    "print(objective_function.parameter_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Configuration\n",
    "Let's use the default optimizer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.BayesianOptimizerConfigStore import bayesian_optimizer_config_store\n",
    "\n",
    "optimizer_config = bayesian_optimizer_config_store.default\n",
    "print(optimizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Evaluation\n",
    "Let's configure the optimizer evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.OptimizerEvaluationTools.OptimizerEvaluator import OptimizerEvaluator, optimizer_evaluator_config_store\n",
    "\n",
    "evaluator_config = optimizer_evaluator_config_store.default\n",
    "evaluator_config.num_iterations = 200\n",
    "evaluator_config.evaluation_frequency = 10\n",
    "\n",
    "print(evaluator_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the evaluation a bunch of times. We can do it in parallel to save a bit of time.\n",
    "#\n",
    "import concurrent.futures\n",
    "\n",
    "num_runs = 8\n",
    "max_workers = 8\n",
    "\n",
    "\n",
    "evaluation_reports = []\n",
    "with traced(\"parallel_evaluations\"), concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    outstanding_futures = set()\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        evaluator = OptimizerEvaluator(\n",
    "            optimizer_evaluator_config=evaluator_config,\n",
    "            optimizer_config=optimizer_config,\n",
    "            objective_function=objective_function\n",
    "        )\n",
    "        future = executor.submit(evaluator.evaluate_optimizer)\n",
    "        outstanding_futures.add(future)\n",
    "    \n",
    "    done_futures, outstanding_futures = concurrent.futures.wait(outstanding_futures, return_when=concurrent.futures.ALL_COMPLETED)\n",
    "    \n",
    "    for future in done_futures:\n",
    "        optimizer_evaluation_report = future.result()\n",
    "        evaluation_reports.append(optimizer_evaluation_report)\n",
    "        global_values.tracer.trace_events.extend(optimizer_evaluation_report.execution_trace)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_trace_file_path = os.path.join(optimizer_results_dir, \"parallel_evaluation.json\")\n",
    "print(evaluation_trace_file_path)\n",
    "global_values.tracer.dump_trace_to_file(output_file_path=evaluation_trace_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "Let's plot the various optima over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_reports = [optimizer_evaluation_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from mlos.Optimizers.OptimumDefinition import OptimumDefinition\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=(11, 20), dpi=80, sharex=True)\n",
    "\n",
    "optimum_definition_names = [\n",
    "    OptimumDefinition.BEST_OBSERVATION.value,\n",
    "    OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG.value,\n",
    "    f\"{OptimumDefinition.UPPER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG.value}_99\",\n",
    "    f\"{OptimumDefinition.LOWER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG.value}_99\"\n",
    "]\n",
    "optimum_column_names = ['y', 'predicted_value', 'upper_confidence_bound', 'lower_confidence_bound']\n",
    "\n",
    "for i, (optimum_definition_name, optimum_column_name) in enumerate(zip(optimum_definition_names, optimum_column_names)):\n",
    "    for j, report in enumerate(evaluation_reports):\n",
    "        optimum_df = report.optima_over_time[optimum_definition_name].get_dataframe()\n",
    "        if len(optimum_df.index) > 0:\n",
    "            axs[i].plot(optimum_df['iteration'], optimum_df[optimum_column_name], label=j)\n",
    "            axs[i].set_xticks(optimum_df['iteration'])\n",
    "        \n",
    "    axs[i].set_ylabel(optimum_column_name)\n",
    "    axs[i].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "    axs[i].grid(True)\n",
    "    axs[i].set_xlabel('iteration')\n",
    "    axs[i].legend()  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Bayesian Optimizer Evaluation\n",
    "\n",
    "1. Perform optimizer convergence evaluation on some functions out of the factory:\n",
    "    1. Define some named configurations for the BayesianOptimizer (surrogate model config, utility function, utility function optimizer config).\n",
    "    2. Write the optimum object to:\n",
    "        1. Include all observations (at first - once we know what we don't need, we can remove them)\n",
    "        2. Include surrogate model predictions for all observations.\n",
    "        3. Include a dataframe with the various definitions of optimum:\n",
    "            1. Best observed observation.\n",
    "            2. Observation with the highest:\n",
    "                1. predicted mean\n",
    "                2. upper confidence bound\n",
    "                3. lower confidence bound\n",
    "            3. Speculative optima - kick of the utility function optimizer to find configurations with:\n",
    "                1. maximum predicted mean\n",
    "                2. maximum upper confidence bound\n",
    "                3. maximum lower confidence bound\n",
    "        1. Include a dataframe with cummax of all of the 7 optima.\n",
    "        2. Compare the number of iterations needed to reach the same optimum for the various optimizers.\n",
    "        \n",
    "    3. Train the various models on the various functions and plot all 7 optima as a function of a number of observations (repeat each experiment a few times to get the idea of stability).\n",
    "    4. Progressively increase the difficulty of the optimization problems:\n",
    "        1. Amount of noise.\n",
    "        2. Number of dimensions.\n",
    "        3. Discontinuous functions.\n",
    "        4. Search spaces with more branching and nesting.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Utility Function Optimizer Evaluation\n",
    "\n",
    "This is a two-step process.\n",
    "1. Evaluate these model-free optimizers on the ObjectiveFunctionBase subclasses. This should allow us to capture any bugs, and get a sense of how quickly these optimizers converge on a variety of functions.\n",
    "2. Evaluate these model-free optimizers as part of the bayesian optimizer. The key difference is that the model gets refit, so the underlying function changes between optimizer invocations, but we get to use prior invocations as starting points.\n",
    "\n",
    "3. Plot all of this :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
