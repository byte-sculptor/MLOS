{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Smart Cache with Bayesian Optimization\n",
    "\n",
    "The goal of this notebook is to optimize SmartCache using Bayesian Optimization approach.\n",
    "\n",
    "We're using a sequential model-based optimization approach, that consists of the following loop:\n",
    "1. Get suggested config from optimizer,\n",
    "2. Apply suggested config to ``SmartCache``,\n",
    "3. Execute a fixed workload,\n",
    "4. Collect the metrics from ``SmartCache``,\n",
    "5. Register an observation with the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required classes and tools\n",
    "import grpc\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from mlos.Logger import create_logger\n",
    "\n",
    "from mlos.Examples.SmartCache import HitRateMonitor, SmartCache, SmartCacheWorkloadGenerator, SmartCacheWorkloadLauncher\n",
    "from mlos.Mlos.SDK import MlosExperiment\n",
    "from mlos.Optimizers.BayesianOptimizerFactory import BayesianOptimizerFactory\n",
    "from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective\n",
    "from mlos.Spaces import Point, SimpleHypergrid, ContinuousDimension\n",
    "\n",
    "# The optimizer will be in a remote process via grpc, we pick the port here:\n",
    "grpc_port = 50051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the optimizer service in a different process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "optimizer_microservice = subprocess.Popen(f\"start_optimizer_microservice launch --port {grpc_port}\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the optimizer service that runs the surrogate model and suggests new points is started in the background.\n",
    "Next, we instantiate an object that connects to it over grpc using the ``BayesianOptimizerFactory``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger('Optimizing Smart Cache', logging_level=logging.WARN)\n",
    "optimizer_service_grpc_channel = grpc.insecure_channel(f'localhost:{grpc_port}')\n",
    "bayesian_optimizer_factory = BayesianOptimizerFactory(grpc_channel=optimizer_service_grpc_channel, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can instantiate our optimization problem. We want to optimize the configuration of the ``SmartCache`` component that contains two implementations: an LRU (least recently used) cache and an MRU cache (most recently used).\n",
    "The ``SmartCache`` component has two parameters that we can adjust, the type of cache and the cache size. We are using some synthetic workloads for the cache and try to find what the optimum configuration for each workload is.\n",
    "\n",
    "Here, we measure 'optimum' by the number of cache hits. Another option would be to measure runtime; however, this is a toy example with a trivial workload and there is likely substantial runtime difference.\n",
    "The parameter search space is declared in ``SmartCache.parameter_search_space``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmartCache.parameter_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization problem is constructed using this parameter space as the input to optimize, and defines a single continuous objective, 'hit_rate' between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Problem\n",
    "#\n",
    "optimization_problem = OptimizationProblem(\n",
    "    parameter_space=SmartCache.parameter_search_space,\n",
    "    objective_space=SimpleHypergrid(name=\"objectives\", dimensions=[ContinuousDimension(name=\"hit_rate\", min=0, max=1)]),\n",
    "    objectives=[Objective(name=\"hit_rate\", minimize=False)]\n",
    ")\n",
    "# create an optimizer proxy that connects to the remote optimizer via grpc:\n",
    "optimizer = bayesian_optimizer_factory.create_remote_optimizer(optimization_problem=optimization_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining workloads\n",
    "Now we can instantiate our workloads and stand up the MLOS infrastructure, both of which are orchestrated by``SmartCacheWorkloadLauncher``. The MLOS infrastructure consists of the MlosAgent and a communication channel, which are available to both the ``SmartCacheWorkloadGenerator`` and the ``SmartCache``.\n",
    "The ``SmartCacheWorkloadLauncher`` launches workloads in ``SmartCacheWorkloadGenerator`` in a separate thread, which will actually generate and run the workloads for the smart cache.\n",
    "The SmartCacheWorkloadLauncher also connects the ``SmartCacheWorkLloadGenerator`` to the optimization problem via a ``MlosAgent`` that will consume the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_launcher = SmartCacheWorkloadLauncher(logger=logger)\n",
    "mlos_agent = workload_launcher.mlos_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the agent to consume configurations for the ``SmartCacheWorkloadGenerator``, and we configure the workload to be sequential keys from a range from 0 to 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlos_agent.set_configuration(\n",
    "    component_type=SmartCacheWorkloadGenerator,\n",
    "    new_config_values=Point(\n",
    "        workload_type='cyclical_key_from_range',\n",
    "        cyclical_key_from_range_config=Point(\n",
    "            min=0,\n",
    "            range_width=2048\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching the experiment (measurement)\n",
    "Now we build the experiment, which collects hit-rate statistics from the ``SmartCacheWorkloadGenerator`` via the ``HitRateMonitor``. This architecture reflects the native architecture for the C++ interface in which communication is done via shared memory between MLOS and the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate_monitor = HitRateMonitor()\n",
    "smart_cache_experiment = MlosExperiment(\n",
    "    smart_component_types=[SmartCache],\n",
    "    telemetry_aggregators=[hit_rate_monitor]\n",
    ")\n",
    "mlos_agent.start_experiment(smart_cache_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the optimization\n",
    "Now that we have all the pieces in place, we can iterate our main optimization loop.\n",
    "Our workload will run in the same process as this notebook, but in a separate thread, which we block on.\n",
    "In a real example, the workload might run completely independent of our optimization procedure.\n",
    "\n",
    "We run the optimization for 20 iterations, in each of which we obtain a new configuration from the optimizer (that interfaces the remote optimizer service).\n",
    "The configuration is passed to ``SmartCacheWorkloadGenerator`` via the ``MlosAgent``, after which we start a blocking workload for 0.2 seconds.\n",
    "Then, the hit-rate (our objective) is read from the ``HitRateMonitor`` and the suggested configuration together with the resulting hit-rate are passed to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iterations = 100\n",
    "data = []\n",
    "for i in range(num_iterations):\n",
    "    # suggest runs a 'cheap' search on the surrogate model to find a good candidate configuration\n",
    "    new_config_values = optimizer.suggest()\n",
    "    # set_configuration communicates the proposed configuration to the SmartCache\n",
    "    mlos_agent.set_configuration(component_type=SmartCache, new_config_values=new_config_values)\n",
    "    hit_rate_monitor.reset()\n",
    "    # start_workload will actually run the worker, here for 0.2 seconds\n",
    "    workload_launcher.start_workload(duration_s=0.2, block=True)\n",
    "    # obtain hit-rate as quality measure for configuration\n",
    "    hit_rate = hit_rate_monitor.get_hit_rate()\n",
    "    objectives_df = pd.DataFrame({'hit_rate': [hit_rate]})\n",
    "    # pass configuration and observed hit-rate to the optimizer to update the surrogate model\n",
    "    features_df = new_config_values.to_dataframe()\n",
    "    optimizer.register(features_df, objectives_df)\n",
    "    data.append((features_df, objectives_df))\n",
    "    print(f\"[{i+1}/{num_iterations}] current_config: {new_config_values.to_json()}, hit_rate: {hit_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing results\n",
    "\n",
    "For a cyclical workload with 2048 keys, we assume that a MRU cache with a size of at least 2048 will perform best, and get 100% hits once the cache is filled.\n",
    "Now lets see the suggestions and results from the current experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some pandas wrangling\n",
    "\n",
    "features, targets = zip(*data)\n",
    "data = pd.concat(features, ignore_index=True)\n",
    "data['hit_rate'] = pd.concat(targets, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by implementation, then plot\n",
    "lru_data, mru_data = data.groupby('implementation')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "line_lru = lru_data[1].plot(x='lru_cache_config.cache_size', y='hit_rate', label='LRU', marker='o', linestyle='none', alpha=.6)\n",
    "mru_data[1].plot(x='mru_cache_config.cache_size', y='hit_rate', label='MRU', marker='o', linestyle='none', alpha=.6, ax=plt.gca())\n",
    "plt.ylabel(\"Cache hitrate\")\n",
    "plt.xlabel(\"Cache Size\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if the cache size is over 2048 keys, it means everything can fit into the cache and the strategy does not matter.\n",
    "However, for smaller cache sizes, the MRU strategy has an obvious advantage over the LRU strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Further\n",
    "\n",
    "1) Log how the optimum evolves over time. How many iterations are needed?\n",
    "\n",
    "2) Can you adjust options in the Optimizer to improve convergence (see the BayesianOptimization notebook for suggestions).\n",
    "\n",
    "3) Choose a different workload in the ``SmartCacheWorkloadGenerator``. How do the workloads change the optimum strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "We need to stop all processes & separate threads after running the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "#\n",
    "mlos_agent.stop_experiment(smart_cache_experiment)\n",
    "mlos_agent.stop_all()\n",
    "\n",
    "# Stop the optimizer service\n",
    "import signal\n",
    "optimizer_microservice.send_signal(signal.SIGTERM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
