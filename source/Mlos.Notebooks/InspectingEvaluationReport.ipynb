{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from IPython.core.display import display, HTML\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import mlos.global_values as global_values\n",
    "from mlos.Logger import create_logger\n",
    "from mlos.Optimizers.BayesianOptimizerFactory import BayesianOptimizerFactory\n",
    "from mlos.Optimizers.BayesianOptimizerConfigStore import bayesian_optimizer_config_store\n",
    "from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective\n",
    "from mlos.Spaces import ContinuousDimension, Point, SimpleHypergrid\n",
    "from mlos.Tracer import Tracer\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "global_values.declare_singletons()\n",
    "global_values.tracer = Tracer(actor_id=\"MetaOptimizer\", thread_id=0)\n",
    "\n",
    "# Let's prepare a directory for our results.\n",
    "#\n",
    "optimizer_results_root_dir = r\"E:\\Mlos\\OptimizerOptimization\"\n",
    "optimizer_results_folder_name = f\"optimization_started_on_{datetime.datetime.now().strftime('%Y_%m_%d_at_%H_%M_%S')}\"\n",
    "optimizer_results_dir = os.path.abspath(os.path.join(optimizer_results_root_dir, optimizer_results_folder_name))\n",
    "evaluation_reports_dir = os.path.abspath(os.path.join(optimizer_results_dir, \"evaluation_reports\"))\n",
    "os.mkdir(optimizer_results_dir)\n",
    "os.mkdir(evaluation_reports_dir)\n",
    "\n",
    "# Let's stand up the Optimizer Microservice\n",
    "#\n",
    "optimizer_factory = BayesianOptimizerFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a meta optimizer.\n",
    "#\n",
    "meta_optimizer_config = bayesian_optimizer_config_store.default # get_config_by_name(\"default_with_glow_worm\")\n",
    "meta_optimizer_config.homogeneous_random_forest_regression_model_config.n_estimators = 50\n",
    "meta_optimizer_config.homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit = 5\n",
    "meta_optimizer_config.experiment_designer_config.fraction_random_suggestions = 0.5\n",
    "meta_optimizer_config.experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration = 10000\n",
    "\n",
    "num_inner_optimizer_iterations = 1000\n",
    "meta_optimizer_objective_name = f\"optimum_value_after_{num_inner_optimizer_iterations}_iterations\"\n",
    "\n",
    "meta_optimizer = optimizer_factory.create_local_optimizer(\n",
    "    optimizer_config=meta_optimizer_config,\n",
    "    optimization_problem=OptimizationProblem(\n",
    "        parameter_space=bayesian_optimizer_config_store.parameter_space,\n",
    "        objective_space=SimpleHypergrid(\n",
    "            name=\"predictions\",\n",
    "            dimensions=[\n",
    "                ContinuousDimension(name=meta_optimizer_objective_name, min=-math.inf, max=math.inf)\n",
    "            ]\n",
    "        ),\n",
    "        objectives=[Objective(name=meta_optimizer_objective_name, minimize=True)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.OptimizerEvaluationTools.ObjectiveFunctionFactory import ObjectiveFunctionFactory\n",
    "from mlos.OptimizerEvaluationTools.ObjectiveFunctionConfigStore import objective_function_config_store\n",
    "\n",
    "objective_function_config = objective_function_config_store.get_config_by_name('5_mutually_exclusive_polynomials')\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.input_domain_min = -20\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.input_domain_width = 40\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.coefficient_domain_min = 100\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.coefficient_domain_width = 10\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.include_noise = False\n",
    "objective_function_config.nested_polynomial_objective_config.polynomial_objective_config.noice_coefficient_of_variation = 0.0\n",
    "print(objective_function_config.to_json(indent=2))\n",
    "\n",
    "objective_function = ObjectiveFunctionFactory.create_objective_function(objective_function_config)\n",
    "print(objective_function.parameter_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Logger import create_logger\n",
    "from mlos.Optimizers.RegressionModels.RegressionModelFitState import RegressionModelFitState\n",
    "\n",
    "# Let us set up the lists to track optima over time.\n",
    "#\n",
    "best_observation_num_observations = []\n",
    "best_observation_configs = []\n",
    "best_observations = []\n",
    "\n",
    "predicted_value_num_observations = []\n",
    "best_predicted_value_configs = []\n",
    "best_predicted_values = []\n",
    "\n",
    "evaluation_reports = []\n",
    "\n",
    "regression_model_fit_state = RegressionModelFitState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(\"Optimizing.\")\n",
    "num_completed_outer_loop_iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_observations_to_file(optimizer, target_dir):\n",
    "    configs, values = optimizer.get_all_observations()    \n",
    "    merged_observations = pd.concat([configs, values], axis=1)\n",
    "    with open(os.path.join(target_dir, f\"{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}combined.csv\"), \"w\") as out_file:\n",
    "        merged_observations.to_csv(out_file, line_terminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "from mlos.Optimizers.OptimumDefinition import OptimumDefinition\n",
    "from mlos.Optimizers.RegressionModels.GoodnessOfFitMetrics import GoodnessOfFitMetrics, DataSetType\n",
    "from mlos.OptimizerEvaluationTools.OptimizerEvaluator import OptimizerEvaluator, optimizer_evaluator_config_store\n",
    "\n",
    "\n",
    "\n",
    "num_desired_runs = 5000\n",
    "num_completed_runs = 0\n",
    "num_failed_runs = 0\n",
    "max_concurrent_jobs = 20 # To keep them all saturated.\n",
    "max_workers = 12\n",
    "\n",
    "\n",
    "optimizer_evaluator_config = optimizer_evaluator_config_store.default\n",
    "optimizer_evaluator_config.num_iterations = num_inner_optimizer_iterations\n",
    "optimizer_evaluator_config.evaluation_frequency = math.ceil(num_inner_optimizer_iterations / 10)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    outstanding_futures = set()\n",
    "    while num_completed_runs < num_desired_runs:\n",
    "        logger.info(f\"Completed: {num_completed_runs}, Desired: {num_desired_runs}, Outstanding: {len(outstanding_futures)}\")\n",
    "\n",
    "        # Let's keep submitting new jobs to the pool until we have the desired number of executions.\n",
    "        #\n",
    "        num_remaining_jobs_to_schedule = num_desired_runs - num_completed_runs - len(outstanding_futures)\n",
    "        num_jobs_to_schedule_now = min(num_remaining_jobs_to_schedule, max_concurrent_jobs - len(outstanding_futures))\n",
    "        if num_jobs_to_schedule_now > 0:\n",
    "            for _ in range(num_jobs_to_schedule_now):\n",
    "                logger.info(f\"Completed: {num_completed_runs}, Desired: {num_desired_runs}, Outstanding: {len(outstanding_futures)}\")\n",
    "                logger.info(f\"Getting suggestion from meta optimizer.\")\n",
    "                #inner_optimizer_config = meta_optimizer.suggest()\n",
    "                inner_optimizer_config = bayesian_optimizer_config_store.default\n",
    "                \n",
    "                logger.info(f\"Completed: {num_completed_runs}, Desired: {num_desired_runs}, Outstanding: {len(outstanding_futures)}\")\n",
    "                logger.info(f\"Inner optimizer config: {inner_optimizer_config.to_json(indent=2)}\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                optimizer_evaluator = OptimizerEvaluator(\n",
    "                    optimizer_evaluator_config=optimizer_evaluator_config,\n",
    "                    objective_function_config=objective_function_config,\n",
    "                    optimizer_config=inner_optimizer_config\n",
    "                )\n",
    "                \n",
    "                future = executor.submit(\n",
    "                    optimizer_evaluator.evaluate_optimizer\n",
    "                )\n",
    "                \n",
    "                outstanding_futures.add(future)\n",
    "\n",
    "        # Now let's wait for any future to complete. \n",
    "        #\n",
    "        logger.info(\"Waiting for futures to complete.\")\n",
    "        logger.info(f\"Completed: {num_completed_runs}, Desired: {num_desired_runs}, Outstanding: {len(outstanding_futures)}\")\n",
    "        done_futures, outstanding_futures = concurrent.futures.wait(outstanding_futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "        logger.info(f\"{len(done_futures)} future(s) completed.\")\n",
    "        logger.info(f\"Completed: {num_completed_runs}, Desired: {num_desired_runs}, Outstanding: {len(outstanding_futures)}\")\n",
    "        \n",
    "        inner_optimizer_configs = []\n",
    "        meta_optimizer_objectives = []\n",
    "        \n",
    "        for future in done_futures:\n",
    "            try:\n",
    "                evaluation_report = future.result()\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                evaluation_report_folder = os.path.join(evaluation_reports_dir, f\"{datetime.datetime.now().strftime('%Y_%m_%d_at_%H_%M_%S.%f')}_{evaluation_report.success}\")\n",
    "                os.mkdir(evaluation_report_folder)\n",
    "                evaluation_report.write_to_disk(target_folder=evaluation_report_folder)\n",
    "                \n",
    "                inner_optimizer_config = evaluation_report.optimizer_configuration\n",
    "                best_observation_config, best_observation_value = evaluation_report.optima_over_time[\"best_observation\"].get_last_optimum()\n",
    "                \n",
    "                inner_optimizer_configs.append(inner_optimizer_config)\n",
    "                meta_optimizer_objectives.append(Point(**{meta_optimizer_objective_name:best_observation_value.y}))\n",
    "                \n",
    "            except:\n",
    "                logger.error(\"Failed to complete inner run.\", exc_info=True)\n",
    "\n",
    "            num_completed_runs += 1\n",
    "            num_completed_outer_loop_iterations += 1\n",
    "        \n",
    "        \n",
    "        inner_optimizer_configs_df = pd.DataFrame([config.to_dict() for config in inner_optimizer_configs])\n",
    "        meta_optimizer_objectives_df = pd.DataFrame([objective.to_dict() for objective in meta_optimizer_objectives])\n",
    "        \n",
    "        assert len(inner_optimizer_configs_df.index) == len(meta_optimizer_objectives_df.index)\n",
    "        \n",
    "        logger.info(f\"Registering {len(meta_optimizer_objectives_df.index)} observations with meta optimizer.\")        \n",
    "        meta_optimizer.register(\n",
    "            feature_values_pandas_frame=inner_optimizer_configs_df,\n",
    "            target_values_pandas_frame=meta_optimizer_objectives_df\n",
    "        )\n",
    "            \n",
    "        ################################################################################################################################\n",
    "        #\n",
    "        #\n",
    "        # TODO: Make these methods on a Convergence State.\n",
    "        #\n",
    "        #\n",
    "        if meta_optimizer.trained:\n",
    "            gof_metrics = meta_optimizer.compute_surrogate_model_goodness_of_fit()\n",
    "            regression_model_fit_state.set_gof_metrics(data_set_type=DataSetType.TRAIN, gof_metrics=gof_metrics)\n",
    "\n",
    "        best_observation_num_observations.append(num_completed_outer_loop_iterations)\n",
    "\n",
    "        try:\n",
    "            best_observation_config, best_observation = meta_optimizer.optimum(OptimumDefinition.BEST_OBSERVATION)    \n",
    "            best_observation_configs.append(best_observation_config)\n",
    "            best_observations.append(best_observation)\n",
    "\n",
    "        \n",
    "            best_predicted_value_config, best_predicted_value = meta_optimizer.optimum(OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG)\n",
    "            best_predicted_value_configs.append(best_predicted_value_config)\n",
    "            best_predicted_values.append(best_predicted_value)\n",
    "            predicted_value_num_observations.append(num_completed_outer_loop_iterations)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        save_observations_to_file(optimizer=meta_optimizer, target_dir=optimizer_results_dir)\n",
    "\n",
    "save_observations_to_file(optimizer=meta_optimizer, target_dir=optimizer_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(evaluation_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in evaluation_reports:\n",
    "    print(report.optima_over_time['best_predicted_value'].get_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluation_reports[2]\n",
    "print(report.regression_model_goodness_of_fit_state.get_goodness_of_fit_dataframe(DataSetType.TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=(11, 20), dpi=80, sharex=True)\n",
    "\n",
    "optimum_definition_names = ['best_observation', 'best_predicted_value', 'ucb_99', 'lcb_99']\n",
    "optimum_column_names = ['y', 'predicted_value', 'upper_confidence_bound', 'lower_confidence_bound']\n",
    "\n",
    "for i, (optimum_definition_name, optimum_column_name) in enumerate(zip(optimum_definition_names, optimum_column_names)):\n",
    "    for j, report in enumerate(evaluation_reports):\n",
    "        optimum_df = report.optima_over_time[optimum_definition_name].get_dataframe()\n",
    "        if len(optimum_df.index) > 0:\n",
    "            axs[i].plot(optimum_df['iteration'], optimum_df[optimum_column_name], label=j)\n",
    "            axs[i].set_xticks(optimum_df['iteration'])\n",
    "        \n",
    "    axs[i].set_ylabel(optimum_column_name)\n",
    "    axs[i].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "    axs[i].grid(True)\n",
    "    axs[i].set_xlabel('iteration')\n",
    "    axs[i].legend()  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_optimizer.get_all_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_observation_config, best_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predicted_value_config, best_predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best observation dataframe\n",
    "#\n",
    "best_observation_df = pd.DataFrame([observation.to_dict() for observation in best_observations])\n",
    "best_observation_df['num_observations'] = best_observation_num_observations\n",
    "best_observation_df = pd.concat([best_observation_df.drop_duplicates(subset=[meta_optimizer_objective_name], keep='last'), best_observation_df.drop_duplicates(subset=[meta_optimizer_objective_name], keep='first')]).sort_index()\n",
    "best_observation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predicted_value_df = pd.DataFrame([predicted_value.to_dict() for predicted_value in best_predicted_values])\n",
    "best_predicted_value_df['num_observations'] = predicted_value_num_observations\n",
    "best_predicted_value_df = pd.concat([best_predicted_value_df.drop_duplicates(subset=['predicted_value'], keep='last'), best_predicted_value_df.drop_duplicates(subset=['predicted_value'], keep='first')]).sort_index()\n",
    "best_predicted_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(11, 20), dpi=80, sharex=True)\n",
    "\n",
    "axs.plot(best_observation_df['num_observations'], best_observation_df[meta_optimizer_objective_name], label=meta_optimizer_objective_name)\n",
    "axs.plot(best_predicted_value_df['num_observations'], best_predicted_value_df['predicted_value'], label='predicted_value')\n",
    "axs.set_ylabel('y')\n",
    "axs.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "axs.set_xticks(best_observation_df['num_observations'][::2])\n",
    "axs.grid(True)\n",
    "axs.set_xlabel('num_observations')\n",
    "axs.legend()  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.RegressionModels.GoodnessOfFitMetrics import DataSetType\n",
    "\n",
    "# Let's take a look at goodness of fit data.\n",
    "#\n",
    "goodness_of_fit_dataframe = regression_model_fit_state.get_goodness_of_fit_dataframe(data_set_type=DataSetType.TRAIN) # TODO: add support to evaluate GoF on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "gof_df = goodness_of_fit_dataframe\n",
    "columns_to_plot = [name for name in gof_df.columns.values if name not in ('observation_count', 'prediction_count', 'last_refit_iteration_number')]\n",
    "num_plots = len(columns_to_plot)\n",
    "fig, axs = plt.subplots(num_plots, figsize=(11, 20), dpi=80, sharex=True)\n",
    "\n",
    "for i, column in enumerate(columns_to_plot):\n",
    "    axs[i].plot(gof_df['last_refit_iteration_number'], gof_df[column], marker='o', label=column)\n",
    "    axs[i].set_ylabel(column)\n",
    "    axs[i].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "    axs[i].set_xticks(gof_df['last_refit_iteration_number'])\n",
    "    axs[i].grid(True)\n",
    "    if i == num_plots - 1:\n",
    "        axs[i].set_xlabel('last_refit_iteration_number')\n",
    "        \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_values.tracer.dump_trace_to_file(r\"E:\\code\\new_mlos\\source\\Mlos.Python\\temp\\meta_optimizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs, values = meta_optimizer.get_all_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "with pd.option_context('display.max_rows', 2000, 'display.max_columns', 100):\n",
    "    display(pd.concat([values, configs], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.OptimumDefinition import OptimumDefinition\n",
    "\n",
    "predicted_best_config, predicted_optimum = meta_optimizer.optimum(OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG)\n",
    "print(predicted_best_config.to_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
