{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import warnings\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the basic principles of [Bayesian Optimization (BO)](https://en.wikipedia.org/wiki/Bayesian_optimization) and how to use MLOS to perform BO.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "In software performance engineering, the impact different (input) parameters (e.g. buffer size, worker thread count, etc.) can have on the (output) performance of a system for a given workload (input) can be modeled as a multidimensional function - one which we don't know the equation for apriori, but are instead trying to learn through careful sampling of the input space and experimentation (test/benchmark runs) to gather output points.\n",
    "Bayesian optimization is one technique for efficiently selecting the samples in the input space to learn the approximate shape of that function and find its optimum, i.,e. the parameters that lead to the best performance.\n",
    "In this example we use a synthetic (i.e. made-up) function that we can look at directly to stand in for a complex system with unknown characteristics.\n",
    "\n",
    "Bayesian Optimization is a [global optimization](https://en.wikipedia.org/wiki/Global_optimization) strategy, so a way to find the global optimum of a mathematical function that's not necessarily [convex](https://en.wikipedia.org/wiki/Convex_function). BO is a black-box optimization technique, meaning that it requires only function values and no other information like gradients.\n",
    "\n",
    "This is in contrast to other optimization strategies, such as gradient descent or conjugate gradient that require gradients and are only guaranteed to find a local optimum (if the function is assumed to be convex, this is also the global optimum).\n",
    "\n",
    "Finding the global optimum of a general non-convex function is NP-hard, which makes it impossible to provide effective convergence guarantees for any global optimization strategy, including Bayesian Optimization. However, BO has been found to be quite effective in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A synthetic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a simple synthetic example of a one-dimensional function that we assume is unknown.\n",
    "If we actually had access to the function, we could use more efficient techniques using calculus and would not be using Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Synthetic 1D objective function.\n",
    "    \n",
    "    This is a sinus envelope, which makes it an almost adversarial example because:\n",
    "        1. There is a lot of local optima.\n",
    "        2. Global maxima are in the vicinity of global minima. So small errors cause massive degradations.\n",
    "        3. This cannot be well approximated by neither linear, nor polynomial regressions.\n",
    "    \n",
    "    \"\"\"\n",
    "    return -(6*x-2)**2*np.sin(12*x-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real use case for global optimization, the function we want to optimize is usually only implicitly defined and very expensive to compute, such as training and evaluating a neural network, or timing the run of a large workload on a distributed database. Given the cost of evaluating the function, our goal is to find an optimum while keeping the number of function evaluations to a minimum.\n",
    "\n",
    "In this synthetic example, we actually know the function, so we can just plot it for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = np.linspace(-10, 10, 1000)\n",
    "values = f(line)\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(line, values)\n",
    "plt.xlabel(\"Input (parameter)\")\n",
    "plt.ylabel(\"Objective (i.e. performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to find the global minimum of this function, assuming that we don't have direct access to the formula (given the formula, we could instead calculate the optimum quite precicely using methods from calculus instead). Usually, the function is too expensive to evaluate in such a manner, in particular in higher-dimensional spaces.\n",
    "\n",
    "Now, we use MLOS to construct an OptimizationProblem object that will encapsulate the function and the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective\n",
    "from mlos.Optimizers.BayesianOptimizer import BayesianOptimizer\n",
    "from mlos.Spaces import SimpleHypergrid, ContinuousDimension\n",
    "\n",
    "\n",
    "input_space = SimpleHypergrid(\n",
    "    name=\"input\",\n",
    "    dimensions=[\n",
    "        ContinuousDimension(name=\"x\", min=-10, max=10)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "output_space = SimpleHypergrid(\n",
    "    name=\"objective\",\n",
    "    dimensions=[\n",
    "        ContinuousDimension(name=\"function_value\", min=-100, max=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "optimization_problem = OptimizationProblem(\n",
    "    parameter_space=input_space,\n",
    "    objective_space=output_space,\n",
    "    objectives=[Objective(name=\"function_value\", minimize=False)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way Bayesian Optimization (in particular what is known as sequential model-based optimization) works is by iterating the following steps:\n",
    "- Evaluate the function at a candidate point x_i (start with a random point x_0), observe f(x_i).\n",
    "- Build / update a **surrogate model** g_i of the objective function (here a Random Forest) using the pairs x_i, f(x_i) that we observed so far.\n",
    "- Pick the next data point to evaluate based on the updated model g_i using a criterion known as **acquisition function**.\n",
    "\n",
    "The idea is that eventually the surrogate model will provide a good approximation of the objective function, but it will be much faster to evaluate (i.e. by predicting with a Random Forest or Gaussian process or another trained machine learning model, instead of running a complex deployment). The acquisition function serves as a means to trade off exploration vs exploitation in collecting new data for building the surrogate model: it picks points that have a low (close to optimum) value of the surrogate model (and so are expected to have a low value of the actual objective). This is the \"exploitation\" of existing knowledge in the model. On the other hand, it also encourages exploring new areas in which there is a lot of uncertainty in the surrogate model, i.e. where we expect the surrogate model not to be very acurate yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is coordinated by the ``BayesianOptimizer`` object, which we will use to perform Bayesian Optimization with a random forest surrogate model. Details of this particular method can be found in [Hutter et. al. (2011)](https://www.cs.ubc.ca/~hutter/papers/11-LION5-SMAC.pdf). We're first configuring the model to refit after every iteration and use 10 trees for the random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.BayesianOptimizerConfigStore import bayesian_optimizer_config_store\n",
    "from mlos.Optimizers.BayesianOptimizerFactory import BayesianOptimizerFactory\n",
    "from mlos.Spaces import Point\n",
    "\n",
    "# Modify the config a bit. \n",
    "#\n",
    "optimizer_config = bayesian_optimizer_config_store.default\n",
    "random_forest_config = optimizer_config.homogeneous_random_forest_regression_model_config\n",
    "\n",
    "# For more complex spaces, this would cause a performance degradation. But since the function is so simple, we can afford to refit the models on every new sample.\n",
    "#\n",
    "optimizer_config.homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit = 1\n",
    "optimizer_config.homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator = .9\n",
    "\n",
    "optimizer_factory = BayesianOptimizerFactory()\n",
    "optimizer = optimizer_factory.create_local_optimizer(\n",
    "    optimization_problem=optimization_problem,\n",
    "    optimizer_config=optimizer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the actual optimization which will carry out the steps outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(optimizer):\n",
    "    \n",
    "    # SUGGEST\n",
    "    #\n",
    "    suggested_value = optimizer.suggest()\n",
    "    input_values_df = suggested_value.to_dataframe()\n",
    "    \n",
    "    # EVALUATE\n",
    "    #\n",
    "    target_value = f(suggested_value['x'])\n",
    "    target_values_df = pd.DataFrame({'function_value': [target_value]})\n",
    "    \n",
    "    # REGISTER\n",
    "    #\n",
    "    print(suggested_value.to_json(), target_value)\n",
    "    optimizer.register(input_values_df, target_values_df)\n",
    "\n",
    "# Run for some iterations.\n",
    "#\n",
    "n_iterations = 25\n",
    "for i in range(n_iterations):\n",
    "    run_optimization(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the hit rate is not always increasing, and that is for two reasons: first, the optimizer keeps exploring parts of the space in which there is uncertainty. Second, by default the BayesianOptimizer picks a fraction of points at random to increase exploration. This fraction is set as ``optimizer_config.experiment_designer_config_fraction_random_suggestions = .1`` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 15 iterations, the model is likely to have captured the general shape, but probably not have found the actual optimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_predictions = optimizer.predict(pd.DataFrame({'x': line})).get_dataframe()\n",
    "feature_values, target_values = optimizer.get_all_observations()\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.scatter(feature_values, target_values, label='observed points')\n",
    "\n",
    "# plot true function (usually unknown)\n",
    "#\n",
    "plt.plot(line, values, label='true function')\n",
    "\n",
    "# plot the surrogate\n",
    "#\n",
    "alpha = optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha\n",
    "t_values = t.ppf(1 - alpha / 2.0, surrogate_predictions['predicted_value_degrees_of_freedom'])\n",
    "ci_radii = t_values * np.sqrt(surrogate_predictions['predicted_value_variance'])\n",
    "value = surrogate_predictions['predicted_value']\n",
    "plt.plot(line, value, label='surrogate predictions g')\n",
    "plt.fill_between(line, value - ci_radii, value + ci_radii, alpha=.1)\n",
    "plt.plot(line, optimizer.experiment_designer.utility_function(pd.DataFrame({'x': line})), ':', label='utility_function')\n",
    "plt.ylabel(\"Objective function f (performance)\")\n",
    "plt.xlabel(\"Input variable\")\n",
    "plt.legend()\n",
    "\n",
    "# Let's add a histogram to show the frequency with which we queried x's.\n",
    "#\n",
    "ax = plt.gca()\n",
    "bins_axes = ax.twinx()\n",
    "bins_axes.set_ylabel(\"Points sampled\")\n",
    "feature_values.hist(bins=20, ax=bins_axes, alpha=.3, color='k', label=\"count of querry points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.OptimumDefinition import OptimumDefinition\n",
    "\n",
    "# Let's plot some optima on top of this plot. Each definition of optimum has some important properties.\n",
    "#\n",
    "\n",
    "# BEST_OBSERVATION - is for those among us who don't care about noise.\n",
    "#\n",
    "best_config_point, best_objective = optimizer.optimum(optimum_definition=OptimumDefinition.BEST_OBSERVATION)\n",
    "\n",
    "# PREDICTED_VALUE_FOR_OBSERVED_CONFIG - for those of us who care about average performance.\n",
    "#\n",
    "predicted_best_config, predicted_optimum = optimizer.optimum(OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG)\n",
    "\n",
    "# UPPER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG - for those of us who are feeling lucky.\n",
    "#\n",
    "ucb_99_ci_config, ucb_99_ci_optimum = optimizer.optimum(OptimumDefinition.UPPER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)\n",
    "\n",
    "# LOWER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG - for those of us who care about the P99.\n",
    "#\n",
    "lcb_99_ci_config, lcb_99_ci_optimum = optimizer.optimum(OptimumDefinition.LOWER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the same thing as above.\n",
    "#\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.scatter(feature_values, target_values, label='observed points')\n",
    "plt.plot(line, values, label='true function')\n",
    "plt.plot(line, value, label='surrogate predictions g')\n",
    "plt.fill_between(line, value - ci_radii, value + ci_radii, alpha=.1)\n",
    "plt.plot(line, optimizer.experiment_designer.utility_function(pd.DataFrame({'x': line})), ':', label='utility_function')\n",
    "\n",
    "# Now let's overlay the various optima.\n",
    "#\n",
    "plt.scatter(best_config_point.x, best_objective.function_value, label='best observation', color='red')\n",
    "plt.scatter(predicted_best_config.x, predicted_optimum.predicted_value, label='predicted optimum')\n",
    "plt.scatter(ucb_99_ci_config.x, ucb_99_ci_optimum.upper_confidence_bound, label=\"upper confidence bound\")\n",
    "plt.scatter(lcb_99_ci_config.x, lcb_99_ci_optimum.lower_confidence_bound, label=\"lower confidence bound\")\n",
    "\n",
    "plt.ylabel(\"Objective function f (performance)\")\n",
    "plt.xlabel(\"Input variable\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run more iterations to improve the surrogate model and the optimum that is found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for more iterations\n",
    "n_iterations = 50\n",
    "for i in range(n_iterations):\n",
    "    run_optimization(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the surrogate model and optimization process again. The points are colored according to the iteration number, with dark blue points being early in the process and yellow points being later. You can see that at the end of the optimization, the points start to cluster around the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Optimizers.OptimumDefinition import OptimumDefinition\n",
    "\n",
    "# Let's plot some optima on top of this plot. Each definition of optimum has some important properties.\n",
    "#\n",
    "\n",
    "# BEST_OBSERVATION - is for those among us who don't care about noise.\n",
    "#\n",
    "best_config_point, best_objective = optimizer.optimum(optimum_definition=OptimumDefinition.BEST_OBSERVATION)\n",
    "\n",
    "# PREDICTED_VALUE_FOR_OBSERVED_CONFIG - for those of us who care about average performance.\n",
    "#\n",
    "predicted_best_config, predicted_optimum = optimizer.optimum(OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG)\n",
    "\n",
    "# UPPER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG - for those of us who are feeling lucky.\n",
    "#\n",
    "ucb_99_ci_config, ucb_99_ci_optimum = optimizer.optimum(OptimumDefinition.UPPER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)\n",
    "\n",
    "# LOWER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG - for those of us who care about the P99.\n",
    "#\n",
    "lcb_99_ci_config, lcb_99_ci_optimum = optimizer.optimum(OptimumDefinition.LOWER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "surrogate_predictions = optimizer.predict(pd.DataFrame({'x': line})).get_dataframe()\n",
    "feature_values, target_values = optimizer.get_all_observations()\n",
    "plt.scatter(feature_values, target_values, label='observed points')\n",
    "plt.plot(line, values, label='true function')\n",
    "alpha = optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha\n",
    "t_values = t.ppf(1 - alpha / 2.0, surrogate_predictions['predicted_value_degrees_of_freedom'])\n",
    "ci_radii = t_values * np.sqrt(surrogate_predictions['predicted_value_variance'])\n",
    "value = surrogate_predictions['predicted_value']\n",
    "plt.plot(line, value, label='surrogate predictions g')\n",
    "plt.fill_between(line, value - ci_radii, value + ci_radii, alpha=.1)\n",
    "\n",
    "# Now let's overlay the various optima.\n",
    "#\n",
    "plt.scatter(best_config_point.x, best_objective.function_value, label='best observation', color='red')\n",
    "plt.scatter(predicted_best_config.x, predicted_optimum.predicted_value, label='predicted optimum')\n",
    "plt.scatter(ucb_99_ci_config.x, ucb_99_ci_optimum.upper_confidence_bound, label=\"upper confidence bound\")\n",
    "plt.scatter(lcb_99_ci_config.x, lcb_99_ci_optimum.lower_confidence_bound, label=\"lower confidence bound\")\n",
    "\n",
    "plt.plot(line, optimizer.experiment_designer.utility_function(pd.DataFrame({'x': line})), ':', label='utility_function')\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Objective function f\")\n",
    "ax.set_xlabel(\"Input variable\")\n",
    "\n",
    "\n",
    "# Let's add a histogram to show the frequency with which we queried x's.\n",
    "#\n",
    "bins_axes = ax.twinx()\n",
    "bins_axes.set_ylabel(\"Points sampled\")\n",
    "feature_values.hist(bins=50, ax=bins_axes, alpha=.3, color='k', label=\"count of query points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for more iterations\n",
    "n_iterations = 50\n",
    "for i in range(n_iterations):\n",
    "    run_optimization(optimizer)\n",
    "\n",
    "# Recompute all optima.\n",
    "#\n",
    "best_config_point, best_objective = optimizer.optimum(optimum_definition=OptimumDefinition.BEST_OBSERVATION)\n",
    "predicted_best_config, predicted_optimum = optimizer.optimum(OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG)\n",
    "ucb_99_ci_config, ucb_99_ci_optimum = optimizer.optimum(OptimumDefinition.UPPER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)\n",
    "lcb_99_ci_config, lcb_99_ci_optimum = optimizer.optimum(OptimumDefinition.LOWER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "surrogate_predictions = optimizer.predict(pd.DataFrame({'x': line})).get_dataframe()\n",
    "feature_values, target_values = optimizer.get_all_observations()\n",
    "plt.scatter(feature_values, target_values, label='observed points')\n",
    "plt.plot(line, values, label='true function')\n",
    "alpha = optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha\n",
    "t_values = t.ppf(1 - alpha / 2.0, surrogate_predictions['predicted_value_degrees_of_freedom'])\n",
    "ci_radii = t_values * np.sqrt(surrogate_predictions['predicted_value_variance'])\n",
    "value = surrogate_predictions['predicted_value']\n",
    "plt.plot(line, value, label='surrogate predictions g')\n",
    "plt.fill_between(line, value - ci_radii, value + ci_radii, alpha=.1)\n",
    "\n",
    "# Now let's overlay the various optima.\n",
    "#\n",
    "plt.scatter(best_config_point.x, best_objective.function_value, label='best observation', color='red')\n",
    "plt.scatter(predicted_best_config.x, predicted_optimum.predicted_value, label='predicted optimum')\n",
    "plt.scatter(ucb_99_ci_config.x, ucb_99_ci_optimum.upper_confidence_bound, label=\"upper confidence bound\")\n",
    "plt.scatter(lcb_99_ci_config.x, lcb_99_ci_optimum.lower_confidence_bound, label=\"lower confidence bound\")\n",
    "\n",
    "plt.plot(line, optimizer.experiment_designer.utility_function(pd.DataFrame({'x': line})), ':', label='utility_function')\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Objective function f\")\n",
    "ax.set_xlabel(\"Input variable\")\n",
    "\n",
    "\n",
    "# Let's add a histogram to show the frequency with which we queried x's.\n",
    "#\n",
    "bins_axes = ax.twinx()\n",
    "bins_axes.set_ylabel(\"Points sampled\")\n",
    "feature_values.hist(bins=50, ax=bins_axes, alpha=.3, color='k', label=\"count of query points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run for more iterations\n",
    "n_iterations = 200\n",
    "for i in range(n_iterations):\n",
    "    run_optimization(optimizer)\n",
    "\n",
    "# Recompute all optima.\n",
    "#\n",
    "best_config_point, best_objective = optimizer.optimum(optimum_definition=OptimumDefinition.BEST_OBSERVATION)\n",
    "predicted_best_config, predicted_optimum = optimizer.optimum(OptimumDefinition.PREDICTED_VALUE_FOR_OBSERVED_CONFIG)\n",
    "ucb_99_ci_config, ucb_99_ci_optimum = optimizer.optimum(OptimumDefinition.UPPER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)\n",
    "lcb_99_ci_config, lcb_99_ci_optimum = optimizer.optimum(OptimumDefinition.LOWER_CONFIDENCE_BOUND_FOR_OBSERVED_CONFIG, alpha=0.01)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "surrogate_predictions = optimizer.predict(pd.DataFrame({'x': line})).get_dataframe()\n",
    "feature_values, target_values = optimizer.get_all_observations()\n",
    "plt.scatter(feature_values, target_values, label='observed points')\n",
    "plt.plot(line, values, label='true function')\n",
    "alpha = optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha\n",
    "t_values = t.ppf(1 - alpha / 2.0, surrogate_predictions['predicted_value_degrees_of_freedom'])\n",
    "ci_radii = t_values * np.sqrt(surrogate_predictions['predicted_value_variance'])\n",
    "value = surrogate_predictions['predicted_value']\n",
    "plt.plot(line, value, label='surrogate predictions g')\n",
    "plt.fill_between(line, value - ci_radii, value + ci_radii, alpha=.1)\n",
    "\n",
    "# Now let's overlay the various optima.\n",
    "#\n",
    "plt.scatter(best_config_point.x, best_objective.function_value, label='best observation', color='red')\n",
    "plt.scatter(predicted_best_config.x, predicted_optimum.predicted_value, label='predicted optimum')\n",
    "plt.scatter(ucb_99_ci_config.x, ucb_99_ci_optimum.upper_confidence_bound, label=\"upper confidence bound\")\n",
    "plt.scatter(lcb_99_ci_config.x, lcb_99_ci_optimum.lower_confidence_bound, label=\"lower confidence bound\")\n",
    "\n",
    "plt.plot(line, optimizer.experiment_designer.utility_function(pd.DataFrame({'x': line})), ':', label='utility_function')\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Objective function f\")\n",
    "ax.set_xlabel(\"Input variable\")\n",
    "\n",
    "\n",
    "# Let's add a histogram to show the frequency with which we queried x's.\n",
    "#\n",
    "bins_axes = ax.twinx()\n",
    "bins_axes.set_ylabel(\"Points sampled\")\n",
    "feature_values.hist(bins=50, ax=bins_axes, alpha=.3, color='k', label=\"count of query points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous Notes\n",
    "\n",
    "Advantages of surrogate model optimization over hill climbing and direct methods:\n",
    "1. Multi-objective\n",
    "2. Resilient to noise\n",
    "3. Non-convex problems, discontinuous functions, categorical parameters.\n",
    "4. Reusable outcomes\n",
    "5. Generating insights\n",
    "\n",
    "Additionally, we have a clever way of describing the search space:\n",
    "1. We preserve component structure.\n",
    "2. We reduce dimensionality for ML models (__massive__ improvements in goodness of fit)\n",
    "\n",
    "\n",
    "Remaining big ticket items:\n",
    "1. Optimizer Evaluation Framework (in-progress)\n",
    "2. Persistence\n",
    "3. Constraints\n",
    "4. Multi-objective optimization\n",
    "5. Post-analysis tools\n",
    "6. Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypergrids..\n",
    "#\n",
    "bayesian_optimizer_config_store.parameter_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "with pd.option_context('display.max_rows', 2000, 'display.max_columns', 100):\n",
    "    display(bayesian_optimizer_config_store.parameter_space.random_dataframe(num_samples=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
